# %% [markdown]
# üöÄ Antigravity Math Solver - Training Notebook (Kaggle/Colab Version)
# Dataset: Neeze/CROHME-full (HuggingFace)

# %% [markdown]
# ## 1. C√†i ƒë·∫∑t th∆∞ vi·ªán

# %%
!pip install -q transformers datasets evaluate jiwer torch torchvision

# %% [markdown]
# ## 2. Khai b√°o th∆∞ vi·ªán & C·∫•u h√¨nh

# %%
import torch
from torch.utils.data import Dataset
from transformers import (
    VisionEncoderDecoderModel,
    ViTImageProcessor,
    AutoTokenizer,
    Seq2SeqTrainer,
    Seq2SeqTrainingArguments,
    default_data_collator,
    TrOCRProcessor
)
from datasets import load_dataset
from PIL import Image
import numpy as np

# === C·∫§U H√åNH ===
# Option 1: 'custom_vit_gpt2' (Encoder: ViT, Decoder: GPT2 from scratch)
# Option 2: 'trocr_finetune' (Ti·∫øp t·ª•c train tr√™n model TrOCR c·ªßa Microsoft) -> KHUY·∫æN NGH·ªä d√πng c√°i n√†y cho nhanh
MODEL_TYPE = "trocr_finetune"

if MODEL_TYPE == "custom_vit_gpt2":
    encoder_checkpoint = "google/vit-base-patch16-224-in21k"
    decoder_checkpoint = "gpt2"
else:
    # Model pre-trained t·ªët nh·∫•t cho vi·∫øt tay
    encoder_checkpoint = "microsoft/trocr-base-handwritten" 
    decoder_checkpoint = "microsoft/trocr-base-handwritten"

max_length = 128
batch_size = 8
epochs = 3
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

print(f"Using device: {device} | Mode: {MODEL_TYPE}")

# %% [markdown]
# ## 3. T·∫£i D·ªØ li·ªáu t·ª´ Hugging Face
# Dataset: https://huggingface.co/datasets/Neeze/CROHME-full

# %%
print("‚è≥ ƒêang t·∫£i dataset Neeze/CROHME-full...")
dataset = load_dataset("Neeze/CROHME-full")

# Ki·ªÉm tra c·∫•u tr√∫c dataset
print(f"Dataset structure: {dataset}")
print(f"Sample: {dataset['train'][0]}")

# N·∫øu dataset ch∆∞a chia train/test, ta t·ª± chia
if "test" not in dataset:
    dataset = dataset["train"].train_test_split(test_size=0.1)

print(f"Train size: {len(dataset['train'])} | Validation size: {len(dataset['test'])}")

# %% [markdown]
# ## 4. X·ª≠ l√Ω d·ªØ li·ªáu (Preprocessing)

# %%
# Load Processor (bao g·ªìm Image Processor v√† Tokenizer)
if MODEL_TYPE == "trocr_finetune":
    processor = TrOCRProcessor.from_pretrained(encoder_checkpoint)
    image_processor = processor.image_processor
    tokenizer = processor.tokenizer
else:
    image_processor = ViTImageProcessor.from_pretrained(encoder_checkpoint)
    tokenizer = AutoTokenizer.from_pretrained(decoder_checkpoint)
    tokenizer.pad_token = tokenizer.eos_token

# H√†m x·ª≠ l√Ω t·ª´ng m·∫´u d·ªØ li·ªáu
def preprocess_function(examples):
    # 1. X·ª≠ l√Ω ·∫£nh: Chuy·ªÉn sang RGB v√† qua image_processor
    # L∆∞u √Ω: 'image' c·ªôt trong dataset l√† PIL Image
    images = [img.convert("RGB") for img in examples["image"]]
    pixel_values = image_processor(images, return_tensors="pt").pixel_values
    
    # 2. X·ª≠ l√Ω text: Tokenize label
    # L∆∞u √Ω: C·∫ßn ki·ªÉm tra t√™n c·ªôt ch·ª©a label LaTeX (th∆∞·ªùng l√† 'latex' ho·∫∑c 'label' ho·∫∑c 'text')
    # ·ªû ƒë√¢y ta th·ª≠ l·∫•y c·ªôt 'latex', n·∫øu kh√¥ng c√≥ th√¨ l·∫•y 'text'
    text_column = "latex" if "latex" in examples else "text"
    if text_column not in examples:
        # Fallback t√¨m c·ªôt ch·ª©a chu·ªói
        available = list(examples.keys())
        text_column = [k for k in available if k != 'image'][0]
    
    texts = examples[text_column]
    
    model_inputs = tokenizer(
        texts, 
        padding="max_length", 
        max_length=max_length, 
        truncation=True
    )
    
    # G√°n -100 cho pad token ƒë·ªÉ kh√¥ng t√≠nh loss
    labels = model_inputs.input_ids
    labels_with_ignore_index = []
    for label_example in labels:
        label_example = [label if label != tokenizer.pad_token_id else -100 for label in label_example]
        labels_with_ignore_index.append(label_example)
    
    model_inputs["pixel_values"] = pixel_values
    model_inputs["labels"] = labels_with_ignore_index
    
    return model_inputs

# √Åp d·ª•ng map function ƒë·ªÉ x·ª≠ l√Ω to√†n b·ªô dataset
# batched=True gi√∫p x·ª≠ l√Ω nhanh h∆°n
print("‚è≥ ƒêang ti·ªÅn x·ª≠ l√Ω d·ªØ li·ªáu...")
processed_dataset = dataset.map(preprocess_function, batched=True, remove_columns=dataset["train"].column_names)
processed_dataset.set_format(type="torch")

print("‚úÖ ƒê√£ x·ª≠ l√Ω xong!")

# %% [markdown]
# ## 5. Kh·ªüi t·∫°o & Train Model

# %%
model = VisionEncoderDecoderModel.from_encoder_decoder_pretrained(
    encoder_checkpoint, 
    decoder_checkpoint
)

# C·∫•u h√¨nh token ƒë·∫∑c bi·ªát
model.config.decoder_start_token_id = tokenizer.bos_token_id if tokenizer.bos_token_id else tokenizer.cls_token_id
model.config.pad_token_id = tokenizer.pad_token_id
model.config.vocab_size = model.config.decoder.vocab_size

# C·∫•u h√¨nh sinh text (Beam Search)
model.config.eos_token_id = tokenizer.eos_token_id
model.config.max_length = max_length
model.config.early_stopping = True
model.config.no_repeat_ngram_size = 3
model.config.length_penalty = 2.0
model.config.num_beams = 4

# ƒê·ªãnh nghƒ©a metric CER (Character Error Rate)
import evaluate
cer_metric = evaluate.load("cer")

def compute_metrics(pred):
    labels_ids = pred.label_ids
    pred_ids = pred.predictions
    
    # Gi·∫£i m√£
    pred_str = tokenizer.batch_decode(pred_ids, skip_special_tokens=True)
    labels_ids[labels_ids == -100] = tokenizer.pad_token_id
    label_str = tokenizer.batch_decode(labels_ids, skip_special_tokens=True)
    
    cer = cer_metric.compute(predictions=pred_str, references=label_str)
    return {"cer": cer}

# C·∫•u h√¨nh tham s·ªë training
training_args = Seq2SeqTrainingArguments(
    output_dir="./math_ocr_results",
    per_device_train_batch_size=batch_size,
    per_device_eval_batch_size=batch_size,
    predict_with_generate=True,
    evaluation_strategy="steps",
    save_steps=1000,
    eval_steps=1000,
    logging_steps=200,
    learning_rate=4e-5,
    num_train_epochs=epochs,
    save_total_limit=2,
    fp16=True, # B·∫≠t Mixed Precision cho GPU
    load_best_model_at_end=True,
    metric_for_best_model="cer",
    report_to="none" # T·∫Øt wandb n·∫øu kh√¥ng d√πng
)

trainer = Seq2SeqTrainer(
    model=model,
    tokenizer=image_processor, # Trick: Truy·ªÅn image_processor v√†o ƒë√¢y ƒë·ªÉ trainer bi·∫øt c√°ch pad ·∫£nh n·∫øu c·∫ßn
    args=training_args,
    compute_metrics=compute_metrics,
    train_dataset=processed_dataset["train"],
    eval_dataset=processed_dataset["test"],
    data_collator=default_data_collator,
)

print("üöÄ B·∫Øt ƒë·∫ßu training...")
trainer.train()

# %% [markdown]
# ## 6. L∆∞u v√† T·∫£i Model

# %%
save_path = "./antigravity_model_final"
trainer.save_model(save_path)
tokenizer.save_pretrained(save_path)
if MODEL_TYPE == "trocr_finetune":
    processor.save_pretrained(save_path)
else:
    image_processor.save_pretrained(save_path)

print(f"Model saved to {save_path}")

# N√©n ƒë·ªÉ download
import shutil
shutil.make_archive('antigravity_model_final', 'zip', save_path)
print("‚úÖ DONE! H√£y t·∫£i file 'antigravity_model_final.zip' v·ªÅ.")
